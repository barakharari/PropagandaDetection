{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BERT from scratch\n",
    "\n",
    "### Resources\n",
    "- https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6\n",
    "\n",
    "### Goals\n",
    "- Get the data\n",
    "- Build a tokenizer\n",
    "- Create an input pipeline\n",
    "- Train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "#### Focusing on:\n",
    "\n",
    "1. political speech or expression of any form by politicians on any public platform\n",
    "2. news media covering politics\n",
    "3. social media covering politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Political Social Media Posts: https://www.kaggle.com/crowdflower/political-social-media-posts\n",
    "US Political Tweets: https://www.kaggle.com/tunguz/united-states-political-tweets\n",
    "Twitter Political Corpus (2009 :/) : https://www.usna.edu/Users/cs/nchamber/data/twitter/\n",
    "Hillary Clinton Emails (2015): https://www.kaggle.com/kaggle/hillary-clinton-emails\n",
    "Politics on Reddit: https://www.kaggle.com/gpreda/politics-on-reddit\n",
    "Trump related Tweets: https://data.world/drstevekramer/trump-twitter-analysis-2017\n",
    "Hillary and Donald Tweets: https://www.kaggle.com/benhamner/clinton-trump-tweets\n",
    "US Politician Twitter Set: https://www.kaggle.com/mrmorj/us-politicians-twitter-dataset\n",
    "DT rally speeches: https://www.kaggle.com/christianlillelund/donald-trumps-rallies\n",
    "DT Reddit Comments: https://www.kaggle.com/amalinow/donald-trump-comments-on-reddit\n",
    "Reddit Politics: https://www.kaggle.com/gpreda/politics-on-reddit\n",
    "WorldNews on Reddit: https://www.kaggle.com/rootuser/worldnews-on-reddit\n",
    "Facebook Politics Pages: https://www.kaggle.com/mrisdal/fact-checking-facebook-politics-pages\n",
    "Political Speech Data: https://digital.lib.hkbu.edu.hk/corpus/\n",
    "Presidential Speeches: https://millercenter.org/the-presidency/presidential-speeches\n",
    "Congressional Speech Data: http://www.cs.cornell.edu/home/llee/data/convote.html\n",
    "Reuters Corpus (2004) : https://trec.nist.gov/data/reuters/reuters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from facebook_scraper import get_posts\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_political_social_media_data(file_path):\n",
    "  posts = []\n",
    "  df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "  for i in tqdm(range(df.shape[0])):\n",
    "    tweet = df.iloc[i]['embed']\n",
    "    soup = BeautifulSoup(tweet)\n",
    "    if soup.find(\"p\"):\n",
    "      posts.append(soup.find(\"p\").text)\n",
    "    else:\n",
    "      fb_link = soup.find(\"div\", class_=\"fb-post\")['data-href']\n",
    "      try:\n",
    "        for p in get_posts(post_urls=[fb_link]):\n",
    "          posts.append(p[\"post_text\"])\n",
    "      except:\n",
    "        print(\"No post_text to fetch...\")\n",
    "  return posts\n",
    "\n",
    "tweets = get_political_social_media_data(file_path=\"data/files/political_social_media.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(posts[random.randint(0,len(posts))])\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "root_path = \n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'data/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'data/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('data/text/oscar_it').glob('**/*.txt')]\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=paths[:5], vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34368ba4908ea1be08ba769dfb7764ab7f8ead2384ebb5604cb86637573696f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
